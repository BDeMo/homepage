---
permalink: /
title: "Mingjia (Samuel Jayden) Shi: An dreamer learning and practicing constantly."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

News
======
<font color="red">Actively applying for a 2025 Fall PhD!</font> If you are interested in a student familiar with theoretical analysis, generative model with extensive industry experiences as well, feel free to [Mail](3101ihs@gmail.com)!

Biography
======
In 2024, I completed my master degree at Sichuan University majored in Artificial Intelligence, right where I had completed my bachelor's degree before, supervised by Prof. [Jiancheng Lv](https://center.dicalab.cn/). The majors of my career in Sichuan University are optimization and distributed learning (e.g., decentralized optimization and federated learning), and it's the second year as an intern student of [NUS HPC-Lab](https://ai.comp.nus.edu.sg/), and I enojoy the challenges and interesting topics here (e.g. efficient AI, generative model, parameter generation and etc.).

Career
======
During the period of study, as an author and reviewer of Top AI conferences and journals, I have appreciated the fascination of research and what I want to do. I pursue further studies, a PhD career, and more insights about the nature of AI and the world. I wanna the difficulties encountered not to be KPIs and dull, but rather my own incompetence.

Research Interests
======
The works in hands are about both theoretical analyses and corresponding methods about Generative Models, LLM Safety Alignment, and Federated Learning.


- **Distributed Learning and Optimization**:
Distributed learning is the last one I majored in. The works explore the heterogeneity composition in federated learning primarily from the perspective of information composition, with methods towards information theory and optimization (e.g., maximum expectation algorithms and Bayesian learning). Also involved in the distributed area are load balancing and communication compression. Participating roles include theory, methodology, engineering and promotion.


- **Efficient AI**:
Efficient AI is the recent major engagements and expected future major directions. To improve efficiency, especially training, in AI applications, the works in hands are mainly about data-centric AI and optimization.


- **Generative Model**:
Works about Generative Model interest me the most recently. The big hitter, generative model well-supported by diffusion theory, bring me back to the wonders of physics. A theoretically grounded approach is always fascinating. There are countless ideas whenever thermodynamics is mentioned, and in the diffusion model I vaguely see the unification of chemical entropy and information entropy on stochastic processes from the Bayesian view.


- **LLM Safety and Privacy**:
Another big hitter, LLM, and its practical generation tasks are also of my interests. Being the moon, this field in has a lot of special phenomena as well as things that need to be explained, all of which titillate the curiosity. Similarly, there are a lot of industrial things that need to be solved, for the sake of sixpence, such as, effiicency, human value alignment and privacy. The variety of empirical phenomena of deep learning at scale is equally fascinating.


Newest Works (Selected)
======

Pre-Print 
------
Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation (**Arxiv**) 
[paper](https://arxiv.org/abs/2407.16139)
X. Wu, J. Niu, X. Liu, **M. Shi**, G. Zhu, S. Tang

A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training (**Arxiv**)
[paper](https://arxiv.org/abs/2405.17403)
K. Wang*, Y. Zhou*, **M. Shi***, Z. Yuan, Y. Shang, X. Peng, H. Zhang, Y. You

Conference
------
Federated CINN Clustering for Accurate Clustered Federated Learning (**ICASSP 2024**)
[paper](https://ieeexplore.ieee.org/abstract/document/10447282/)
Y. Zhou, **M. Shi**, Y. Tian, Y. Li, Q. Ye, J. Lv

PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning (**NeurIPS 2023**)
[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5a3674849d6d6d23ac088b9a2552f323-Abstract-Conference.html)
**M. Shi**, Y. Zhou, K. Wang, H. Zhang, S. Huang, Q. Ye, J. Lv

Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence (**ICCV 2023**)
[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Communication-efficient_Federated_Learning_with_Single-Step_Synthetic_Features_Compressor_for_Faster_ICCV_2023_paper.html)
Y. Zhou, **M. Shi**, Y. Li, Y. Sun, Q. Ye, J. Lv

Unconstrained Feature Model and Its General Geometric Patterns in Federated Learning: Local Subspace Minority Collapse (**ICONIP 2023**)
[paper](https://link.springer.com/chapter/10.1007/978-981-99-8132-8_34)
**M. Shi**, Y. Zhou, Q. Ye, J. Lv

Journal
------
DeFTA: A Plug-and-Play Peer-to-Peer Decentralized Federated Learning Framework (**InfoSci**)
[paper](https://www.sciencedirect.com/science/article/pii/S002002552400495X)
Y. Zhou, **M. Shi**, Y. Tian, Q. Ye, J. Lv

DLB: a dynamic load balance strategy for distributed training of deep neural networks (**Trans.ETCI**)
[paper](https://ieeexplore.ieee.org/abstract/document/9960865/)
Q. Ye, Y. Zhou, **M. Shi**, Y. Sun, J. Lv

FLSGD: free local SGD with parallel synchronization (**JoSc**)
[paper](https://link.springer.com/article/10.1007/s11227-021-04267-5)
Q. Ye, Y. Zhou, **M. Shi**, J. Lv