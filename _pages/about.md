---
permalink: /
title: "Mingjia (Samuel Jayden) Shi's Homepage. "
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

 Here's the page (academic CV) about Mingjia (Samuel Jayden) Shi, 石明佳 in Chinese. An ENFP who shouldn't say too much, but especially wants to. If you think the one in the sidebar is too casual, I have a [formal version](images/a_pic_of_mine_2.jpg). You should focus more on the content below than here! Feel free to contact me for collaboration, discussion or just to say hi!

**👨‍🎓 Biography**
---

- Internship in [HoumoAI](https://www.houmoai.com/), researching on topics related to real-world and industrial applications (e.g., efficient AI) here brings me new challendges here.
- It's the 2nd year (written in 2024) as an intern student in [NUS HPC-Lab](https://ai.comp.nus.edu.sg/), and I enojoy the challenges and interesting topics here (e.g. efficient AI, generative model, parameter generation and etc.).
- From Sep. 2021 to June. 2024, I completed my master degree at Sichuan University majored in Artificial Intelligence, right where I had completed my 4-year bachelor's degree before, supervised by Prof. [Jiancheng Lv](https://center.dicalab.cn/). The majors of my career in Sichuan University are distributed optimization and learning (e.g., decentralized optimization and federated learning).


**🎉 Latest News**
---

- [Jan. 25] 25 Fall PhD and internship in my gap year are decided. Interesting collaborations are still welcome.

<details>
<summary>Old ones before 2025</summary>
<br>

[Dec. 24] Waiting for 2025 Fall PhD and projects in my gap year.
<br>
[Aug. 24] Actively applying for a 2025 Fall PhD! If you are interested in a student familiar with theoretical analysis, generative model with extensive industry experiences as well, feel free to mail!

</details>

**👣 Current Career**
---

During my research period, as an author and a reviewer of Top AI conferences and journals, I have appreciated the fascination and what I want to do, so I pursue a PhD career further. I am busy with my visa now.

**🔍 Research Interests**
---

My research focus on the AI efficiency and ...
- 🖐️ **Works in hands**. My works are mainly both theoretical analyses and corresponding methods about Efficient AI on Trends, Generative Models, AI Privacy and Safety and Federated Learning.
- 🎓 **Overall background**. A knowledge and research background about math+cs, system/control/information theories, deep learning thoeries, optimization and generalization.
- 🌟 **More-interest**. There is a continuing interest in technical research as well as basic science research. Physics and other science disciplines are always beautiful.
<!-- - **Distributed Learning and Optimization**: -->
<!-- Distributed learning is the last one I majored in. The works explore the heterogeneity composition in federated learning primarily from the perspective of information composition, with methods towards information theory and optimization. -->
<!-- - **Efficient AI**: -->
<!-- Efficient AI is the recent major engagements and expected future major directions. To improve efficiency, especially training, in AI applications, the works in hands are mainly about data-centric AI and optimization. -->
<!-- - **Generative Model**: -->
<!-- Works about Generative Model interest me the most recently. The big hitter, generative model well-supported by diffusion theory, bring me back to the wonders of physics. A theoretically grounded approach is always fascinating. -->
<!-- - **AI Safety and Privacy**: -->
<!-- Another big hitter, LLM, and its practical generation tasks are also of my interests. A lot of industrial issues that need to be solved, effiicency, human value alignment and privacy. -->


**📄 Selected Publications**
---

📅 **2025**:

1. **CVPR 2025** A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training.
K. Wang\*, <u>M. Shi</u>\*, Y. Zhou, Z. Li, Z. Yuan, Y. Shang, X. Peng, H. Zhang, Y. You
([paper](https://arxiv.org/abs/2405.17403), [code](https://github.com/NUS-HPC-AI-Lab/SpeeD))
<!-- **CVPR 2025** -->

2. **CVPR 2025** Ferret: An Efficient Online Continual Learning Framework under Varying Memory Constraints.
Y. Zhou, Y. Tian, J. Lv, <u>M. Shi</u>, Y. Li, Q. Ye, S. Zhang, J. Lv
([paper]() and [code]() to be released)
<!-- **CVPR 2025** -->

3. **TNNLS** E-3SFC: Communication-Efficient Federated Learning With Double-Way Features Synthesizing.
Y. Zhou*, Y. Tian*, <u>M. Shi</u>, Y. Li, Y. Sun, Q. Ye, J. Lv
([paper](https://arxiv.org/pdf/2502.03092), [code]() to be released)


📅 **Early Selected**:

*Released Pre-Print*
1. **Arxiv** Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training.
<u>M. Shi</u>\*, Y. Zhou*, R. Yu, Z. Li, Z. Liang, X. Zhao, X. Peng, T Rajpurohit, R. Vedantam, W. Zhao, K. Wang, Y. You.
([paper](https://arxiv.org/abs/2412.12496), [code](https://github.com/NUS-HPC-AI-Lab/R-MeeTo), [page](https://bdemo.github.io/R-MeeTo/))
<!-- **Arxiv** -->
1. **Arxiv** Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation.
X. Wu, J. Niu, X. Liu, <u>M. Shi</u>, G. Zhu, S. Tang
([paper](https://arxiv.org/abs/2407.16139))
<!-- **Arxiv** -->

*Conference*
1. **ICASSP 2024** Federated CINN Clustering for Accurate Clustered Federated Learning.
Y. Zhou, <u>M. Shi</u>, Y. Tian, Y. Li, Q. Ye, J. Lv ([paper](https://ieeexplore.ieee.org/abstract/document/10447282/))
<!-- **ICASSP 2024** -->
1. **NeurIPS 2023** PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning.
<u>M. Shi</u>, Y. Zhou, K. Wang, H. Zhang, S. Huang, Q. Ye, J. Lv ([paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5a3674849d6d6d23ac088b9a2552f323-Abstract-Conference.html), [code](https://github.com/BDeMo/pFedBreD_public))
<!-- **NeurIPS 2023** -->
1. **ICONIP 2023** Unconstrained Feature Model and Its General Geometric Patterns in Federated Learning: Local Subspace Minority Collapse.
<u>M. Shi</u>, Y. Zhou, Q. Ye, J. Lv ([paper](https://link.springer.com/chapter/10.1007/978-981-99-8132-8_34))
<!-- **ICONIP 2023** -->
1. **ICCV 2023** Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence.
Y. Zhou, <u>M. Shi</u>, Y. Li, Y. Sun, Q. Ye, J. Lv ([paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Communication-efficient_Federated_Learning_with_Single-Step_Synthetic_Features_Compressor_for_Faster_ICCV_2023_paper.html))
<!-- **ICCV 2023** -->

*Journal*
1. **InfoSci** DeFTA: A Plug-and-Play Peer-to-Peer Decentralized Federated Learning Framework.
Y. Zhou, <u>M. Shi</u>, Y. Tian, Q. Ye, J. Lv ([paper](https://www.sciencedirect.com/science/article/pii/S002002552400495X))
<!-- **InfoSci** -->
1. **Trans.ETCI** DLB: a dynamic load balance strategy for distributed training of deep neural networks.
Q. Ye, Y. Zhou, <u>M. Shi</u>, Y. Sun, J. Lv ([paper](https://ieeexplore.ieee.org/abstract/document/9960865/))
<!-- **Trans.ETCI** -->
1. **JoSc** FLSGD: free local SGD with parallel synchronization.
Q. Ye, Y. Zhou, <u>M. Shi</u>, J. Lv ([paper](https://link.springer.com/article/10.1007/s11227-021-04267-5))
<!-- **JoSc** -->

[Please see the full list in Google Scholar](https://scholar.google.com/citations?user=B6f3ImkAAAAJ)

**🛎 Services**
---

Reviewer: NeurIPs, ICLR, ICML, CVPR, ICCV, MM, ICPP, ICASSP, ICONIP; TNNLS, TCSVT, TPDS, TKDE, Neurocomputing and others.

**🎈 Hobbies**
---

📷 Photograph,
⛺ Travel,
🎵 Music,
🏸 Badminton,
⚽ Football (Soccer),
♟️ Chess and other Table-top Games,
🎱 Billiards,
🎮 Games,
🔬 Study others besides AI.,