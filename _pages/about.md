---
permalink: /
title: "Mingjia (Samuel Jayden) Shi: An dreamer learning and practicing constantly."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

News
======
<font color="red">Actively applying for a 2025 Fall PhD!</font> If you are interested in a student familiar with theoretical analysis, generative model with extensive industry experiences as well, feel free to [Mail](3101ihs@gmail.com)!

Biography
======
In 2024, I completed my master degree at Sichuan University majored in Artificial Intelligence, right where I had completed my bachelor's degree before, supervised by Prof. [Jiancheng Lv](https://center.dicalab.cn/). The majors of my career in Sichuan University are optimization and distributed learning (e.g., decentralized optimization and federated learning), and it's the second year as an intern student of [NUS HPC-Lab](https://ai.comp.nus.edu.sg/), and I enojoy the challenges and interesting topics here (e.g. efficient AI, generative model, parameter generation and etc.).

Career
======
During the period of study, as an author and reviewer of Top AI conferences and journals, I have appreciated the fascination of research and what I want to do, so I pursue further studies, a PhD career. Hope the difficulties encountered not to be KPIs and dull, but rather my own incompetence.

Research Interests
======
The works in hands are about both theoretical analyses and corresponding methods about Generative Models, LLM Safety Alignment, and Federated Learning.


- **Distributed Learning and Optimization**:
Distributed learning is the last one I majored in. The works explore the heterogeneity composition in federated learning primarily from the perspective of information composition, with methods towards information theory and optimization.

- **Efficient AI**:
Efficient AI is the recent major engagements and expected future major directions. To improve efficiency, especially training, in AI applications, the works in hands are mainly about data-centric AI and optimization.


- **Generative Model**:
Works about Generative Model interest me the most recently. The big hitter, generative model well-supported by diffusion theory, bring me back to the wonders of physics. A theoretically grounded approach is always fascinating.

- **LLM Safety and Privacy**:
Another big hitter, LLM, and its practical generation tasks are also of my interests. A lot of industrial issues that need to be solved, effiicency, human value alignment and privacy.

Newest Works (Selected)
======

Pre-Print 
------
**Arxiv** Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation 
X. Wu, J. Niu, X. Liu, **M. Shi**, G. Zhu, S. Tang
([paper](https://arxiv.org/abs/2407.16139))

**Arxiv** A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training 
K. Wang\*, Y. Zhou\*, **M. Shi\***, Z. Yuan, Y. Shang, X. Peng, H. Zhang, Y. You
([paper](https://arxiv.org/abs/2405.17403))

Conference
------
**ICASSP 2024** Federated CINN Clustering for Accurate Clustered Federated Learning 
Y. Zhou, **M. Shi**, Y. Tian, Y. Li, Q. Ye, J. Lv
([paper](https://ieeexplore.ieee.org/abstract/document/10447282/))

**NeurIPS 2023** PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning 
**M. Shi**, Y. Zhou, K. Wang, H. Zhang, S. Huang, Q. Ye, J. Lv
([paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5a3674849d6d6d23ac088b9a2552f323-Abstract-Conference.html))

**ICONIP 2023** Unconstrained Feature Model and Its General Geometric Patterns in Federated Learning: Local Subspace Minority Collapse 
**M. Shi**, Y. Zhou, Q. Ye, J. Lv
([paper](https://link.springer.com/chapter/10.1007/978-981-99-8132-8_34))

**ICCV 2023** Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence 
Y. Zhou, **M. Shi**, Y. Li, Y. Sun, Q. Ye, J. Lv
([paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Communication-efficient_Federated_Learning_with_Single-Step_Synthetic_Features_Compressor_for_Faster_ICCV_2023_paper.html))



Journal
------
**InfoSci** DeFTA: A Plug-and-Play Peer-to-Peer Decentralized Federated Learning Framework 
Y. Zhou, **M. Shi**, Y. Tian, Q. Ye, J. Lv
([paper](https://www.sciencedirect.com/science/article/pii/S002002552400495X))

**Trans.ETCI** DLB: a dynamic load balance strategy for distributed training of deep neural networks 
Q. Ye, Y. Zhou, **M. Shi**, Y. Sun, J. Lv
([paper](https://ieeexplore.ieee.org/abstract/document/9960865/))

**JoSc** FLSGD: free local SGD with parallel synchronization 
Q. Ye, Y. Zhou, **M. Shi**, J. Lv
([paper](https://link.springer.com/article/10.1007/s11227-021-04267-5))