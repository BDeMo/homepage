---
permalink: /
title: "Mingjia (Samuel Jayden) Shi's Homepage. "
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

If you think the left ones are too casual, I have a [formal](images/a_pic_of_mine_2.jpg) version too. You should focus more on the content below than here!

Biography
======
- It's the 2nd year (written in 2024) as an intern student in [NUS HPC-Lab](https://ai.comp.nus.edu.sg/), and I enojoy the challenges and interesting topics here (e.g. efficient AI, generative model, parameter generation and etc.).
- From Sep. 2021 to June. 2024, I completed my master degree at Sichuan University majored in Artificial Intelligence, right where I had completed my 4-year bachelor's degree before, supervised by Prof. [Jiancheng Lv](https://center.dicalab.cn/). The majors of my career in Sichuan University are distributed optimization and learning (e.g., decentralized optimization and federated learning).

Latest News
======
- **DDLs After**. <font color="red">Waiting for 2025 Fall PhD Interviews.</font>
- **DDLs Before**. Actively applying for a 2025 Fall PhD! If you are interested in a student familiar with theoretical analysis, generative model with extensive industry experiences as well, feel free to [ Mail](3101ihs@gmail.com)!

Current Career
======
During my research period, as an author and a reviewer of Top AI conferences and journals, I have appreciated the fascination and what I want to do, so I pursue a PhD career further.


Research Interests
======
- **Works in hands**. My works are mainly both theoretical analyses and corresponding methods about Efficient AI on Trends, Generative Models, AI Privacy and Safety and Federated Learning.
- **Overall background**. A knowledge and research background about math+cs, system/control/information theory, deep learning thoeries, optimization and generalization.
- **Addition**. There is a continuing interest in technical research as well as basic science research. Physics and other science disciplines are always beautiful.
<!-- - **Distributed Learning and Optimization**: -->
<!-- Distributed learning is the last one I majored in. The works explore the heterogeneity composition in federated learning primarily from the perspective of information composition, with methods towards information theory and optimization. -->
<!-- - **Efficient AI**: -->
<!-- Efficient AI is the recent major engagements and expected future major directions. To improve efficiency, especially training, in AI applications, the works in hands are mainly about data-centric AI and optimization. -->
<!-- - **Generative Model**: -->
<!-- Works about Generative Model interest me the most recently. The big hitter, generative model well-supported by diffusion theory, bring me back to the wonders of physics. A theoretically grounded approach is always fascinating. -->
<!-- - **AI Safety and Privacy**: -->
<!-- Another big hitter, LLM, and its practical generation tasks are also of my interests. A lot of industrial issues that need to be solved, effiicency, human value alignment and privacy. -->

Selected Publications
======

2024 and Before
------

[Released Pre-Print]
1. **Arxiv** Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training. Arxiv. **M. Shi**\*, Y. Zhou*, R. Yu, Z. Li, Z. Liang, X. Zhao, X. Peng, T Rajpurohit, R. Vedantam, W. Zhao, K. Wang, Y. You.
([paper](https://arxiv.org/abs/2412.12496), [code](https://github.com/NUS-HPC-AI-Lab/R-MeeTo), [page](https://bdemo.github.io/R-MeeTo/))
1. **Arxiv** Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation.
X. Wu, J. Niu, X. Liu, **M. Shi**, G. Zhu, S. Tang
([paper](https://arxiv.org/abs/2407.16139))
1. **Arxiv** A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training.
K. Wang\*, **M. Shi**\*, Y. Zhou\*, Z. Yuan, Y. Shang, X. Peng, H. Zhang, Y. You
([paper](https://arxiv.org/abs/2405.17403), [code](https://github.com/NUS-HPC-AI-Lab/SpeeD))

[Conference]
1. **ICASSP 2024** Federated CINN Clustering for Accurate Clustered Federated Learning. Y. Zhou, **M. Shi**, Y. Tian, Y. Li, Q. Ye, J. Lv ([paper](https://ieeexplore.ieee.org/abstract/document/10447282/))
1. **NeurIPS 2023** PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. **M. Shi**, Y. Zhou, K. Wang, H. Zhang, S. Huang, Q. Ye, J. Lv ([paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5a3674849d6d6d23ac088b9a2552f323-Abstract-Conference.html), [code](https://github.com/BDeMo/pFedBreD_public))
1. **ICONIP 2023** Unconstrained Feature Model and Its General Geometric Patterns in Federated Learning: Local Subspace Minority Collapse. **M. Shi**, Y. Zhou, Q. Ye, J. Lv ([paper](https://link.springer.com/chapter/10.1007/978-981-99-8132-8_34))
1. **ICCV 2023** Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence. Y. Zhou, **M. Shi**, Y. Li, Y. Sun, Q. Ye, J. Lv ([paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Communication-efficient_Federated_Learning_with_Single-Step_Synthetic_Features_Compressor_for_Faster_ICCV_2023_paper.html))

[Journal]
1. **InfoSci** DeFTA: A Plug-and-Play Peer-to-Peer Decentralized Federated Learning Framework. Y. Zhou, **M. Shi**, Y. Tian, Q. Ye, J. Lv ([paper](https://www.sciencedirect.com/science/article/pii/S002002552400495X))
1. **Trans.ETCI** DLB: a dynamic load balance strategy for distributed training of deep neural networks. Q. Ye, Y. Zhou, **M. Shi**, Y. Sun, J. Lv ([paper](https://ieeexplore.ieee.org/abstract/document/9960865/))
1. **JoSc** FLSGD: free local SGD with parallel synchronization. Q. Ye, Y. Zhou, **M. Shi**, J. Lv ([paper](https://link.springer.com/article/10.1007/s11227-021-04267-5))