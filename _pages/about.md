---
permalink: /
title: "Mingjia (Samuel Jayden) Shi's Homepage. "
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

 Here's the page (academic CV) about Mingjia (Samuel Jayden) Shi, çŸ³æ˜ä½³ in Chinese. An ENFP who shouldn't say too much, but especially wants to. If you think the one in the sidebar is too casual, I have a [formal version](images/a_pic_of_mine_2.jpg). You should focus more on the content below than here! Feel free to contact me for collaboration, discussion or just to say hi!

**ğŸ‘¨â€ğŸ“ Biography**
---

- Internship in [HoumoAI](https://www.houmoai.com/), researching on topics related to real-world and industrial applications (e.g., efficient AI) here brings me new challendges here.
- It's the 2nd year (written in 2024) as an intern student in [NUS HPC-Lab](https://ai.comp.nus.edu.sg/), and I enojoy the challenges and interesting topics here (e.g. efficient AI, generative model, parameter generation and etc.).
- From Sep. 2021 to June. 2024, I completed my master degree at Sichuan University majored in Artificial Intelligence, right where I had completed my 4-year bachelor's degree before, supervised by Prof. [Jiancheng Lv](https://center.dicalab.cn/). The majors of my career in Sichuan University are distributed optimization and learning (e.g., decentralized optimization and federated learning).


**ğŸ‰ Latest News**
---

- [Jan. 25] 25 Fall PhD and internship in my gap year are decided. Interesting collaborations are still welcome.

<details>
<summary>Old ones before 2025</summary>
<br>

[Dec. 24] Waiting for 2025 Fall PhD and projects in my gap year.
<br>
[Aug. 24] Actively applying for a 2025 Fall PhD! If you are interested in a student familiar with theoretical analysis, generative model with extensive industry experiences as well, feel free to mail!

</details>

**ğŸ‘£ Current Career**
---

During my research period, as an author and a reviewer of Top AI conferences and journals, I have appreciated the fascination and what I want to do, so I pursue a PhD career further. I am busy with my visa now.

**ğŸ” Research Interests**
---

My research focus on the AI efficiency and ...
- ğŸ–ï¸ **Works in hands**. My works are mainly both theoretical analyses and corresponding methods about Efficient AI on Trends, Generative Models, AI Privacy and Safety and Federated Learning.
- ğŸ“ **Overall background**. A knowledge and research background about math+cs, system/control/information theories, deep learning thoeries, optimization and generalization.
- ğŸŒŸ **More-interest**. There is a continuing interest in technical research as well as basic science research. Physics and other science disciplines are always beautiful.
<!-- - **Distributed Learning and Optimization**: -->
<!-- Distributed learning is the last one I majored in. The works explore the heterogeneity composition in federated learning primarily from the perspective of information composition, with methods towards information theory and optimization. -->
<!-- - **Efficient AI**: -->
<!-- Efficient AI is the recent major engagements and expected future major directions. To improve efficiency, especially training, in AI applications, the works in hands are mainly about data-centric AI and optimization. -->
<!-- - **Generative Model**: -->
<!-- Works about Generative Model interest me the most recently. The big hitter, generative model well-supported by diffusion theory, bring me back to the wonders of physics. A theoretically grounded approach is always fascinating. -->
<!-- - **AI Safety and Privacy**: -->
<!-- Another big hitter, LLM, and its practical generation tasks are also of my interests. A lot of industrial issues that need to be solved, effiicency, human value alignment and privacy. -->


**ğŸ“„ Selected Publications**
---

ğŸ“… **2025**:

1. **CVPR 2025** A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training.
K. Wang\*, <u>M. Shi</u>\*, Y. Zhou, Z. Li, Z. Yuan, Y. Shang, X. Peng, H. Zhang, Y. You
([paper](https://arxiv.org/abs/2405.17403), [code](https://github.com/NUS-HPC-AI-Lab/SpeeD))
<!-- **CVPR 2025** -->

2. **CVPR 2025** Ferret: An Efficient Online Continual Learning Framework under Varying Memory Constraints.
Y. Zhou, Y. Tian, J. Lv, <u>M. Shi</u>, Y. Li, Q. Ye, S. Zhang, J. Lv
([paper]() and [code]() to be released)
<!-- **CVPR 2025** -->

3. **TNNLS** E-3SFC: Communication-Efficient Federated Learning With Double-Way Features Synthesizing.
Y. Zhou*, Y. Tian*, <u>M. Shi</u>, Y. Li, Y. Sun, Q. Ye, J. Lv
([paper](https://arxiv.org/pdf/2502.03092), [code]() to be released)


ğŸ“… **Early Selected**:

*Released Pre-Print*
1. **Arxiv** Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training.
<u>M. Shi</u>\*, Y. Zhou*, R. Yu, Z. Li, Z. Liang, X. Zhao, X. Peng, T Rajpurohit, R. Vedantam, W. Zhao, K. Wang, Y. You.
([paper](https://arxiv.org/abs/2412.12496), [code](https://github.com/NUS-HPC-AI-Lab/R-MeeTo), [page](https://bdemo.github.io/R-MeeTo/))
<!-- **Arxiv** -->
1. **Arxiv** Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation.
X. Wu, J. Niu, X. Liu, <u>M. Shi</u>, G. Zhu, S. Tang
([paper](https://arxiv.org/abs/2407.16139))
<!-- **Arxiv** -->

*Conference*
1. **ICASSP 2024** Federated CINN Clustering for Accurate Clustered Federated Learning.
Y. Zhou, <u>M. Shi</u>, Y. Tian, Y. Li, Q. Ye, J. Lv ([paper](https://ieeexplore.ieee.org/abstract/document/10447282/))
<!-- **ICASSP 2024** -->
1. **NeurIPS 2023** PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning.
<u>M. Shi</u>, Y. Zhou, K. Wang, H. Zhang, S. Huang, Q. Ye, J. Lv ([paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5a3674849d6d6d23ac088b9a2552f323-Abstract-Conference.html), [code](https://github.com/BDeMo/pFedBreD_public))
<!-- **NeurIPS 2023** -->
1. **ICONIP 2023** Unconstrained Feature Model and Its General Geometric Patterns in Federated Learning: Local Subspace Minority Collapse.
<u>M. Shi</u>, Y. Zhou, Q. Ye, J. Lv ([paper](https://link.springer.com/chapter/10.1007/978-981-99-8132-8_34))
<!-- **ICONIP 2023** -->
1. **ICCV 2023** Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence.
Y. Zhou, <u>M. Shi</u>, Y. Li, Y. Sun, Q. Ye, J. Lv ([paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Communication-efficient_Federated_Learning_with_Single-Step_Synthetic_Features_Compressor_for_Faster_ICCV_2023_paper.html))
<!-- **ICCV 2023** -->

*Journal*
1. **InfoSci** DeFTA: A Plug-and-Play Peer-to-Peer Decentralized Federated Learning Framework.
Y. Zhou, <u>M. Shi</u>, Y. Tian, Q. Ye, J. Lv ([paper](https://www.sciencedirect.com/science/article/pii/S002002552400495X))
<!-- **InfoSci** -->
1. **Trans.ETCI** DLB: a dynamic load balance strategy for distributed training of deep neural networks.
Q. Ye, Y. Zhou, <u>M. Shi</u>, Y. Sun, J. Lv ([paper](https://ieeexplore.ieee.org/abstract/document/9960865/))
<!-- **Trans.ETCI** -->
1. **JoSc** FLSGD: free local SGD with parallel synchronization.
Q. Ye, Y. Zhou, <u>M. Shi</u>, J. Lv ([paper](https://link.springer.com/article/10.1007/s11227-021-04267-5))
<!-- **JoSc** -->

[Please see the full list in Google Scholar](https://scholar.google.com/citations?user=B6f3ImkAAAAJ)

**ğŸ› Services**
---

Reviewer: NeurIPs, ICLR, ICML, CVPR, ICCV, MM, ICPP, ICASSP, ICONIP; TNNLS, TCSVT, TPDS, TKDE, Neurocomputing and others.

**ğŸˆ Hobbies**
---

ğŸ“· Photograph,
â›º Travel,
ğŸµ Music,
ğŸ¸ Badminton,
âš½ Football (Soccer),
â™Ÿï¸ Chess and other Table-top Games,
ğŸ± Billiards,
ğŸ® Games,
ğŸ”¬ Study others besides AI.,