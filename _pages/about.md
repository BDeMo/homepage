---
permalink: /
title: "Mingjia (Samuel Jayden) Shi's Homepage. "
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

 Here's the page (academic CV, [pdf](https://drive.google.com/file/d/1qrYLWORjBR_m0zuJatvB-F-KfKVPNttA/view?usp=sharing)) about Mingjia (Samuel Jayden) Shi, Áü≥Êòé‰Ω≥ in Chinese. An ENFP who shouldn't say too much, but especially wants to. If you think the one in the sidebar is too casual, I have a [formal version](images/a_pic_of_mine_2.jpg). You should focus more on the content below than here! Feel free to contact me for collaboration, discussion or just to say hi!

**üë®‚Äçüéì Biography**
---

- Ph.D. career begins in the VAST Lab, University of Virginia, with the greatest advisor ever of the whole mankind, [Jundong Li](https://jundongli.github.io/index.html).
- Internship in [HoumoAI](https://www.houmoai.com/) (ended in July, 2025), researching on topics related to real-world and industrial applications (e.g., resource-preserving AI) here brings me new challendges here.
<!-- - It's the 2nd year (written in 2024) as an intern student in [NUS HPC-Lab](https://ai.comp.nus.edu.sg/), and I enojoy the challenges and interesting topics here (e.g. efficient AI, generative model, parameter generation and etc.). -->
- It's the 2nd year (written in 2024) as an intern student in [NUS HPC-Lab](https://ai.comp.nus.edu.sg/), and I enojoy the challenges and interesting topics here (e.g. Resource Preserving and Efficiency.).
<!-- - From Sep. 2021 to June. 2024, I completed my master degree at Sichuan University majored in Artificial Intelligence, right where I had completed my 4-year bachelor's degree before, supervised by Prof. [Jiancheng Lv](https://center.dicalab.cn/). The majors of my career in Sichuan University are distributed optimization and learning (e.g., decentralized optimization and federated learning). -->
- From Sep. 2021 to June. 2024, I completed my master degree at Sichuan University majored in Artificial Intelligence (Data Science), right where I had completed my 4-year bachelor's degree before, supervised by Prof. [Jiancheng Lv](https://center.dicalab.cn/). The majors of my career in Sichuan University are federated learning (i.e., Decentralized Data Analyses and Large System).


**üéâ Latest News**
---

- [Aug. 25] Ph.D. in computer engineering career has begun in UVa! Welcome N.A. proposal, paper and all other academic collaborations.

<details>
<summary>Old ones before 2025</summary>
<br>
[Jan. 25] 25 Fall PhD and internship in my gap year are decided. Interesting collaborations are still welcome.
<br>
[Dec. 24] Waiting for 2025 Fall PhD and projects in my gap year.
<br>
[Aug. 24] Actively applying for a 2025 Fall PhD! If you are interested in a student familiar with theoretical analysis, generative model with extensive industry experiences as well, feel free to mail!
<br>
[Aug. 24] Actively applying for a 2025 Fall PhD! If you are interested in a student familiar with theoretical analysis, feel free to mail!

</details>

**üë£ Current Career**
---

Settled down in University of Virginia, Charlottesville. Charlottesville is a beautiful town of great natural and academical environment. I am now exploring new research directions for my following Ph.D. career. Proposal collaborations are all welcomed.

<!-- During my research period, as an author and a reviewer of Top conferences and journals, I have appreciated the fascination and what I want to do, so I pursue a PhD career further. I am busy with my visa now. -->

**üîç Research Interests**
---

My research focus on Resource Preserving and ...
<!-- - üñêÔ∏è **Works in hands**. My works are mainly both theoretical analyses and corresponding methods about Efficient AI on Trends, Generative Models, AI Privacy and Safety and Federated Learning. -->
- üñêÔ∏è **Works in hands**. My works are mainly both theoretical analyses and corresponding methods about Resource Preserving. Multi-Modal and Efficiency on Trends. Besides those only collaborative works before my enrollment in UVa that are kept processing, and the other future works and collaboration are in U.S. or other nonsensitive countries.
- üéì **Overall background**. A knowledge and research background about math+cs, system/control/information theories, deep learning thoeries, optimization and generalization analyses.
- üåü **More-interest**. There is a continuing interest in technical research as well as basic science research. Physics and other science disciplines are always beautiful.
<!-- - **Distributed Learning and Optimization**: -->
<!-- Distributed learning is the last one I majored in. The works explore the heterogeneity composition in federated learning primarily from the perspective of information composition, with methods towards information theory and optimization. -->
<!-- - **Efficient AI**: -->
<!-- Efficient AI is the recent major engagements and expected future major directions. To improve efficiency, especially training, in AI applications, the works in hands are mainly about data-centric AI and optimization. -->
<!-- - **Generative Model**: -->
<!-- Works about Generative Model interest me the most recently. The big hitter, generative model well-supported by diffusion theory, bring me back to the wonders of physics. A theoretically grounded approach is always fascinating. -->
<!-- - **AI Safety and Privacy**: -->
<!-- Another big hitter, LLM, and its practical generation tasks are also of my interests. A lot of industrial issues that need to be solved, effiicency, human value alignment and privacy. -->


**üìÑ Selected Publications**
---

üìÖ **2025**:

1. **NeurIPS 2025** Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights.
Z. Liang, D. Tang, Y. Zhou, X. Zhao, <u>M. Shi,</u> W. Zhao, Z. Li, P. Wang, K. Sch√ºrholt, D. Borth, M. M. Bronstein, Y. You, Z. Wang, K. Wang
([paper](https://arxiv.org/abs/2506.16406))
1. **Arxiv 2025** DD-ranking: Rethinking the evaluation of dataset distillation.
Z. Li, X. Zhong, S. Khaki, Z. Liang, Y. Zhou, <u>M. Shi,</u> Z. Wang, X. Zhao, W. Zhao, Z. Qin, M. Wu, P. Zhou, H. Wang, D. J. Zhang, J. Liu, S. Wang, D. Liu, L. Zhang, G. Li, K. Wang, Z. Zhu, Z. Ma, J. T. Zhou, J. Lv, Y. Jin, P. Wang, K. Zhang, L. Lyu, Y. Huang, Z. Akata, Z. Deng, X. Wu, G. Cazenavette, Y. Shang, J. Cui, J. Gu, Q. Zheng, H. Ye, S. Wang, X. Wang, Y. Yan, A. Yao, M. Z. Shou, T. Chen, H. Bilen, B. Mirzasoleiman, M. Kellis, K. N. Plataniotis, Z. Wang, B. Zhao, Y. You, K. Wang
([paper](https://arxiv.org/abs/2505.13300))
1. **NeurIPS 2025** Pruning-Robust Mamba with Asymmetric Multi-Scale Scanning Paths. J. Lv, Y. Zhou, M. Shi, Z. Liang, P.  Zhang, X. Peng, W. Zhao, Z. Zhu, J. Lv, Q. Ye, K. Wang
([paper](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=B6f3ImkAAAAJ&citation_for_view=B6f3ImkAAAAJ:XiSMed-E-HIC))
1. **NeurIPS 2025** Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation.
X. Wu, J. Niu, X. Liu, <u>M. Shi</u>, G. Zhu, S. Tang
([paper](https://arxiv.org/abs/2407.16139))
1. **NeurIPS 2025** REPA Works Until It Does not: Early-Stopped, Holistic Alignment Supercharges Diffusion Training.
Z. Wang, W. Zhao, Y. Zhou, Z. Li, Z. Liang, <u>M. Shi,</u> X. Zhao, P. Zhou, K. Zhang, Z. Wang, K. Wang, Y. You
([paper](https://arxiv.org/abs/2505.16792))
1. **Arxiv 2025** Make Optimization Once and for All with Fine-grained Guidance. Arxiv. <u>M. Shi,</u> R. Lin, X. Chen, Y. Zhou, Z. Ding, P. Li, T. Wang, K. Wang, Z. Wang, J. Zhang, T. Chen.
([paper](https://arxiv.org/abs/2503.11462))
1. **CVPR 2025** A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training.
K. Wang\*, <u>M. Shi</u>\*, Y. Zhou, Z. Li, Z. Yuan, Y. Shang, X. Peng, H. Zhang, Y. You
([paper](https://arxiv.org/abs/2405.17403), [code](https://github.com/NUS-HPC-AI-Lab/SpeeD))
<!-- **CVPR 2025** -->
1. **CVPR 2025** Ferret: An Efficient Online Continual Learning Framework under Varying Memory Constraints.
Y. Zhou, Y. Tian, J. Lv, <u>M. Shi</u>, Y. Li, Q. Ye, S. Zhang, J. Lv
([paper](https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Ferret_An_Efficient_Online_Continual_Learning_Framework_under_Varying_Memory_CVPR_2025_paper.html))
<!-- **CVPR 2025** -->
1. **TNNLS** E-3SFC: Communication-Efficient Federated Learning With Double-Way Features Synthesizing.
Y. Zhou*, Y. Tian*, <u>M. Shi</u>, Y. Li, Y. Sun, Q. Ye, J. Lv
([paper](https://arxiv.org/pdf/2502.03092))
1. **ACL 2025 Findings** GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning. S. Zhou*, S. Wang*, Z. Yuan*, <u>M. Shi,</u> Y. Shang, D. Yang ([paper](https://arxiv.org/abs/2502.12913))


üìÖ **Early Selected**:

*Released Pre-Print*
1. **Arxiv 2024** Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training.
<u>M. Shi</u>\*, Y. Zhou*, R. Yu, Z. Li, Z. Liang, X. Zhao, X. Peng, T Rajpurohit, R. Vedantam, W. Zhao, K. Wang, Y. You.
([paper](https://arxiv.org/abs/2412.12496), [code](https://github.com/NUS-HPC-AI-Lab/R-MeeTo), [page](https://bdemo.github.io/R-MeeTo/))
<!-- **Arxiv** -->
<!-- 1. **Arxiv 2024** Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation.
X. Wu, J. Niu, X. Liu, <u>M. Shi</u>, G. Zhu, S. Tang
([paper](https://arxiv.org/abs/2407.16139)) -->
<!-- **Arxiv** -->

*Conference*
1. **ICASSP 2024** Federated CINN Clustering for Accurate Clustered Federated Learning.
Y. Zhou, <u>M. Shi</u>, Y. Tian, Y. Li, Q. Ye, J. Lv ([paper](https://ieeexplore.ieee.org/abstract/document/10447282/))
<!-- **ICASSP 2024** -->
1. **NeurIPS 2023** PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning.
<u>M. Shi</u>, Y. Zhou, K. Wang, H. Zhang, S. Huang, Q. Ye, J. Lv ([paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5a3674849d6d6d23ac088b9a2552f323-Abstract-Conference.html), [code](https://github.com/BDeMo/pFedBreD_public))
<!-- **NeurIPS 2023** -->
1. **ICONIP 2023** Unconstrained Feature Model and Its General Geometric Patterns in Federated Learning: Local Subspace Minority Collapse.
<u>M. Shi</u>, Y. Zhou, Q. Ye, J. Lv ([paper](https://link.springer.com/chapter/10.1007/978-981-99-8132-8_34))
<!-- **ICONIP 2023** -->
1. **ICCV 2023** Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence.
Y. Zhou, <u>M. Shi</u>, Y. Li, Y. Sun, Q. Ye, J. Lv ([paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Communication-efficient_Federated_Learning_with_Single-Step_Synthetic_Features_Compressor_for_Faster_ICCV_2023_paper.html))
<!-- **ICCV 2023** -->

*Journal*
1. **InfoSci** DeFTA: A Plug-and-Play Peer-to-Peer Decentralized Federated Learning Framework.
Y. Zhou, <u>M. Shi</u>, Y. Tian, Q. Ye, J. Lv ([paper](https://www.sciencedirect.com/science/article/pii/S002002552400495X))
<!-- **InfoSci** -->
1. **Trans.ETCI** DLB: a dynamic load balance strategy for distributed training of deep neural networks.
Q. Ye, Y. Zhou, <u>M. Shi</u>, Y. Sun, J. Lv ([paper](https://ieeexplore.ieee.org/abstract/document/9960865/))
<!-- **Trans.ETCI** -->
1. **JoSc** FLSGD: free local SGD with parallel synchronization.
Q. Ye, Y. Zhou, <u>M. Shi</u>, J. Lv ([paper](https://link.springer.com/article/10.1007/s11227-021-04267-5))
<!-- **JoSc** -->

[Please see the full list in Google Scholar](https://scholar.google.com/citations?user=B6f3ImkAAAAJ)

**üõé Services**
---

Reviewer: NeurIPS, ICLR, ICML, CVPR, ICCV, MM, ICPP, ICASSP, ICONIP; TNNLS, TCSVT, TPDS, TKDE, Neurocomputing and others.

**üéà Hobbies**
---

üì∑ Photograph,
‚õ∫ Travel,
üéµ Music,
üè∏ Badminton,
‚öΩ Football (Soccer),
‚ôüÔ∏è Chess and other Table-top Games,
üé± Billiards,
üéÆ Games,
üî¨ Study on interesting topics,